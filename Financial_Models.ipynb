{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Financial Models",
      "provenance": [],
      "authorship_tag": "ABX9TyNFDx+3E4sxDw5Ox08al+Rx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H8fPPGNxSrh"
      },
      "source": [
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas_datareader as web\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA as sklearnPCA\n",
        "# !pip install arch\n",
        "from arch import arch_model\n",
        "\n",
        "# !pip install git+https://github.com/RJT1990/pyflux\n",
        "# import pyflux as pf\n",
        "\n",
        "plt.style.use('Solarize_Light2')\n",
        "\n",
        "# Model evaluation = Visual check, log liklihood"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWK0dknsxb0A"
      },
      "source": [
        "\"\"\" Allows web scraping \"\"\"\n",
        "ticker_list = [\"AMT\", \"AZRE\", \"CAT\", \"COST\", \"GOOD\", \"HON\", \"ILMN\", \"NVTA\", \"NEE\", \"PLUG\", \"PLD\", \"UNP\", \"VRTX\", \"Z\"]\n",
        "stockStartDate = '2016-01-01'\n",
        "today = datetime.today().strftime('%Y-%m-%d')\n",
        "numAssets = len(ticker_list)\n",
        "\n",
        "\n",
        "def getMyPortfolio(stocks, start, end, col):\n",
        "    data = web.DataReader(stocks, data_source='yahoo', start=start, end=end)[col]\n",
        "    ticker_df = pd.DataFrame(data)\n",
        "    ticker_df.describe()             # Mean, std, min, 25%, 50%, 75%, max\n",
        "    ticker_df.pct_change().cov()     # Covariance matrix\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "getMyPortfolio(ticker_list, stockStartDate, today, 'Adj Close')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCjb0vrxxe--"
      },
      "source": [
        "\"\"\" Read from csv, plot, write to csv \"\"\"\n",
        "labels_list = ['Asset 1', 'Asset 2', 'Asset 3']\n",
        "\n",
        "data_df = pd.read_csv('Individual_return-1.csv')\n",
        "data_pct = data_df.pct_change()\n",
        "data_pct.to_csv('SP500_pct.csv')\n",
        "print(data_df)\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.title('Plotted csv data')\n",
        "plt.ylabel('Close Price USD')\n",
        "plt.xlabel('Time Period')\n",
        "plt.plot(data_df, lw=1.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz3zMfl987Ha"
      },
      "source": [
        "\"\"\" Autocorrelation - correlation of a signal with a delayed copy of \n",
        "itself as a function of delay. The similarity between observations as a \n",
        "function of the time lag between them.\n",
        "Partial autocorrelation is the autocorrelation between yt and yt–h after the \n",
        "removal of any linear dependence on y1, y2, ..., yt–h+1\n",
        "\"\"\" \n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "series = pd.read_csv('PYPL.csv')\n",
        "plot_pacf(series)\n",
        "plot_acf(series)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVfBgu5-RTbx"
      },
      "source": [
        "\"\"\" ARMA - Autoregressive–moving-average model.\n",
        "AR - Attempts to explain mean reversion behaviour, and takes param 'P'\n",
        "MA - Attempts to capture shock movements within white noise fluctuations\n",
        "and takes param 'Q'\n",
        " \"\"\"\n",
        "\n",
        "# !pip install git+https://github.com/statsmodels/statsmodels\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "series = pd.read_csv('PYPL.csv')          # Must be single column pandas series, not dataframe \n",
        "sq_price = series.pct_change() ** 2\n",
        "\n",
        "train = price.iloc[1:3000]\n",
        "test = price.iloc[3001:]\n",
        "\n",
        "model = ARIMA(train, order=(0, 0, 1), )         # Order = (p, d, q), for all ARIMA-type models\n",
        "model_fit = model.fit()\n",
        "print(model_fit.summary())\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(sq_price)\n",
        "plt.title(\"Squared Returns less Resid. \", fontsize=20)\n",
        "\n",
        "residuals = model_fit.resid\n",
        "plot_2 = price - residuals\n",
        "plt.plot(plot_2)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q36nvDyw0c66"
      },
      "source": [
        "\"\"\" GARCH  - Estimates generalized autoregressive conditional \n",
        "heteroskedasticity. It is an ARCH model used to estimate volatility, using a \n",
        "moving average. This requires a clear variance in volatility, or high levels of \n",
        "volatility clustering. \"\"\"\n",
        "\n",
        "data = web.DataReader('JPM', 'yahoo', stockStartDate, today)    # Replace with real data\n",
        "returns = pd.DataFrame(data['Adj Close']).pct_change()\n",
        "data = returns.iloc[2:]                                         # Remove first NaN\n",
        "\n",
        "am = arch_model(data, p=1, q=1)                                 # GARCH(p,q)\n",
        "res = am.fit(update_freq=0)                                     # Iterative\n",
        "res.summary()\n",
        "# fig = res.plot(annualize=\"D\")                                 # Daily weekly or monthy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67djqkdNxjlJ"
      },
      "source": [
        "\"\"\" \n",
        "Solve a PCA  - Principal component analysis, has 3 steps:\n",
        "1 - Standardization = z = (value -mean)/ standard deviation \n",
        "2 - Covariance matrix computation \n",
        "3 - Eigenveectors, eigenvalues and as such; principal components\n",
        "\n",
        "In notes: \n",
        "- Given a covariance matrix apply PCA to obtain the eigenvalues, \n",
        "principal components, and the transition matrix.\n",
        "- Given a time series, knows how to apply PCA\n",
        "\n",
        "This code tackles step 3, and takes a DataFrame of returns.\n",
        " \"\"\"\n",
        "\n",
        "data_df = pd.read_csv('Individual_return-1.csv')\n",
        "\n",
        "X = data_df\n",
        "pca = sklearnPCA(n_components=3)                                  # Components variable\n",
        "model = pca.fit(X)\n",
        "print(f'Variance Ratio: {model.explained_variance_ratio_}')       # Ratio of principal component (not necessary)\n",
        "print(f'Raw Value Variances: \\n{model.explained_variance_}')      # Raw value variances\n",
        "print(f'Transition Matrix: \\n{model.components_.T}')              # Transition matrix (coeffs)\n",
        "\n",
        "scores = pca.score_samples(X)\n",
        "print(f'Principal Components: \\n{scores}')                        # Prinipal Components\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VALK3Px82KTM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}